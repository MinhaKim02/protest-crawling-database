#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SMPA(ì„œìš¸ê²½ì°°ì²­) 'ì˜¤ëŠ˜ì˜ ì§‘íšŒ' PDF â†’ CSV
- ì˜¤ëŠ˜ì ê²Œì‹œê¸€ PDF ìë™ ë‹¤ìš´ë¡œë“œ(--pdf ë¯¸ì§€ì •) ë˜ëŠ” ë¡œì»¬ PDF ì§€ì •(--pdf)
- ê²Œì‹œê¸€ ì œëª©ì—ì„œ YYMMDD ì¶”ì¶œ â†’ ['ë…„','ì›”','ì¼'] ì±„ì›€ (ë¡œì»¬ PDF ì§€ì • ì‹œ ê³µë€ ìœ ì§€)
- PDF íŒŒì‹±(í–‰ ë‹¨ìœ„)
- VWorld ì§€ì˜¤ì½”ë”©ìœ¼ë¡œ 'ìœ„ë„','ê²½ë„'ë¥¼ JSON ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ì±„ì›€(ì¥ì†Œ ìˆ˜ì™€ ë™ì¼ ê¸¸ì´, ë¯¸ë§¤ì¹­ì€ null)
- ì „ì²´ CSVì™€ 'ì¢…ë¡œ' í•„í„° CSV(ë™ì¼ ì»¬ëŸ¼) 2ê°œ ì €ì¥

ì‚¬ìš© ì˜ˆ:
  python smpa_pdf_to_csv.py                         # ìë™ ë‹¤ìš´ë¡œë“œ, ì§€ì˜¤ì½”ë”©, í˜„ì¬ í´ë” ì €ì¥
  python smpa_pdf_to_csv.py --out ì§‘íšŒì •ë³´.csv      # ì €ì¥ íŒŒì¼ëª… ì§€ì •
  python smpa_pdf_to_csv.py --pdf 250822.pdf        # ë¡œì»¬ PDF ì‚¬ìš©
  python smpa_pdf_to_csv.py --vworld-key <í‚¤>       # VWorld í‚¤ ì§€ì •
"""

import re
import os
import csv
import json
import time
import argparse
import pathlib
import urllib.parse
import datetime
from typing import List, Dict, Optional, Tuple, Any
import pandas as pd

import requests
from bs4 import BeautifulSoup

# pdfminer.six í•„ìš”
try:
    from pdfminer_high_level import extract_text  # intentional failover name
except Exception:
    try:
        from pdfminer.high_level import extract_text
    except ImportError as e:
        raise SystemExit("pdfminer.sixê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ í›„: pip install pdfminer.six") from e

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SMPA(ì„œìš¸ê²½ì°°ì²­) ëª©ë¡/ì²¨ë¶€ PDF ë‹¤ìš´ë¡œë“œ
BASE = "https://www.smpa.go.kr"
LIST_URL = f"{BASE}/user/nd54882.do" # ì„œìš¸ê²½ì°°ì²­ > ì˜¤ëŠ˜ì˜ ì§‘íšŒ


def ensure_dir(p: str):
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)


def sanitize_filename(name: str, limit: int = 120) -> str:
    safe = re.sub(r'[^\wê°€-í£\.-]+', '_', name)
    return safe[:limit].strip('._')


def filename_from_cd(cd: str) -> Optional[str]:
    if not cd:
        return None
    m_star = re.search(r"filename\*\s*=\s*[^']*'[^']*'([^;]+)", cd, re.I)
    if m_star:
        return urllib.parse.unquote(m_star.group(1))
    m = re.search(r'filename\s*=\s*"([^"]+)"', cd, re.I)
    if m:
        return m.group(1)
    m2 = re.search(r'filename\s*=\s*([^;]+)', cd, re.I)
    if m2:
        return m2.group(1).strip()
    return None


def _current_title_pattern() -> Tuple[str, str]:
    # ì˜ˆ) "ì˜¤ëŠ˜ì˜ ì§‘íšŒ 250822 ê¸ˆ"
    from datetime import datetime
    current_date = datetime.now().strftime("%y%m%d")
    weekdays = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
    current_day = weekdays[datetime.now().weekday()]
    return current_date, f"ì˜¤ëŠ˜ì˜ ì§‘íšŒ {current_date} {current_day}"


def parse_goBoardView(href: str):
    m = re.search(r"goBoardView\('([^']+)'\s*,\s*'([^']+)'\s*,\s*'(\d+)'\)", href)
    if not m:
        return None
    return m.group(1), m.group(2), m.group(3)


def build_view_urls(board_no: str) -> List[str]:
    return [
        f"{BASE}/user/nd54882.do?View&boardNo={board_no}",
        f"{BASE}/user/nd54882.do?dmlType=View&boardNo={board_no}",
    ]


def extract_ymd_from_title(title: str) -> Optional[Tuple[str, str, str]]:
    """
    ì œëª©ì—ì„œ YYMMDDë¥¼ ì°¾ì•„ 20YY-MM-DDë¡œ í™•ì¥í•˜ì—¬ ('YYYY','MM','DD') ë°˜í™˜.
    ì˜ˆ) 'ì˜¤ëŠ˜ì˜ ì§‘íšŒ 250822 ê¸ˆ' â†’ ('2025','08','22')
    """
    if not title:
        return None
    m = re.search(r'(\d{2})(\d{2})(\d{2})', title)
    if not m:
        return None
    yy, mm, dd = m.group(1), m.group(2), m.group(3)
    yyyy = f"20{yy}"
    return (yyyy, mm, dd)


def get_today_post_info(session: requests.Session, list_url: str = LIST_URL) -> Tuple[str, str]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ì˜¤ëŠ˜ì ê²Œì‹œê¸€ì˜ ë·° URLê³¼ 'ì œëª© í…ìŠ¤íŠ¸'ë¥¼ í•¨ê»˜ ë°˜í™˜.
      return: (view_url, title_text)
    """
    current_date, expected_full = _current_title_pattern()
    r = session.get(list_url, timeout=20)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    tbody = soup.select_one("#subContents > div > div.inContent > table > tbody")
    targets = tbody.select("a[href^='javascript:goBoardView']") if tbody \
        else soup.select("a[href^='javascript:goBoardView']")

    target_link = None
    target_title = None
    for a in targets:
        title = a.get_text(strip=True) or (a.find_parent('td').get_text(strip=True) if a.find_parent('td') else "")
        href = a.get("href", "")
        if expected_full in title or f"ì˜¤ëŠ˜ì˜ ì§‘íšŒ {current_date}" in title:
            target_link = href
            target_title = title
            break

    if not target_link:
        raise RuntimeError("ì˜¤ëŠ˜ ë‚ ì§œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    parsed = parse_goBoardView(target_link)
    if not parsed:
        raise RuntimeError("goBoardView ì¸ìë¥¼ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    _, _, board_no = parsed

    for url in build_view_urls(board_no):
        resp = session.get(url, timeout=20)
        if resp.ok and "html" in (resp.headers.get("Content-Type") or "").lower():
            return url, (target_title or "")
    raise RuntimeError("View í˜ì´ì§€ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


def parse_attach_onclick(a_tag):
    oc = a_tag.get("onclick", "")
    m = re.search(r"attachfileDownload\('([^']+)'\s*,\s*'(\d+)'\)", oc)
    if not m:
        return None
    return m.group(1), m.group(2)


def _is_pdf(resp: requests.Response, first: bytes) -> bool:
    ct = (resp.headers.get("Content-Type") or "").lower()
    return first.startswith(b"%PDF-") or "pdf" in ct


def download_from_view(session: requests.Session, view_url: str, out_dir: str) -> str:
    ensure_dir(out_dir)
    r = session.get(view_url, timeout=20)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    candidates = []
    for a in soup.find_all("a"):
        oc = a.get("onclick", "")
        if "attachfileDownload" in oc:
            txt = (a.get_text(strip=True) or "").lower()
            if "pdf" in txt or ".pdf" in txt:
                candidates.append(a)
    if not candidates:
        candidates = [a for a in soup.find_all("a") if "attachfileDownload" in (a.get("onclick", "") or "")]

    last_error = None
    for a_tag in candidates:
        parsed = parse_attach_onclick(a_tag)
        if not parsed:
            continue
        path, attach_no = parsed
        download_url = urllib.parse.urljoin(BASE, path)
        try:
            with session.get(download_url, params={"attachNo": attach_no}, stream=True, timeout=30) as resp:
                resp.raise_for_status()
                it = resp.iter_content(chunk_size=8192)
                first_chunk = next(it, b"")
                if not _is_pdf(resp, first_chunk):
                    continue
                cd = resp.headers.get("Content-Disposition", "")
                filename = filename_from_cd(cd) or (a_tag.get_text(strip=True) or f"{attach_no}.pdf")
                root, ext = os.path.splitext(filename)
                if ext.lower() != ".pdf":
                    filename = root + ".pdf"
                filename = sanitize_filename(filename)
                save_path = os.path.join(out_dir, filename)
                with open(save_path, "wb") as f:
                    if first_chunk:
                        f.write(first_chunk)
                    for chunk in it:
                        if chunk:
                            f.write(chunk)
                return save_path
        except Exception as e:
            last_error = e
            continue
    if last_error:
        raise last_error
    raise RuntimeError("PDF ì²¨ë¶€ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


def download_today_pdf_with_title(out_dir: str = "attachments") -> Tuple[str, str]:
    """
    ì˜¤ëŠ˜ì ê²Œì‹œê¸€ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , 'ì œëª© í…ìŠ¤íŠ¸'ë¥¼ í•¨ê»˜ ë°˜í™˜.
      return: (pdf_path, title_text)
    """
    sess = requests.Session()
    sess.headers.update({
        'User-Agent': 'Mozilla/5.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Referer': LIST_URL,
    })
    view_url, title_text = get_today_post_info(sess, LIST_URL)
    pdf_path = download_from_view(sess, view_url, out_dir=out_dir)
    return pdf_path, title_text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF íŒŒì„œ(í–‰ ë‹¨ìœ„)
TIME_RE = re.compile(
    r'(?P<start>\d{1,2}\s*:\s*\d{2})\s*~\s*(?P<end>\d{1,2}\s*:\s*\d{2})',
    re.DOTALL
)

def _normalize_time_breaks(text: str) -> str:
    t = text
    t = re.sub(r'(\d{1,2})\s*\n\s*:\s*(\d{2})', r'\1:\2', t)  # "18\n:00" â†’ "18:00"
    t = re.sub(r'(\d{1,2}\s*:\s*\d{2})\s*\n\s*~\s*\n\s*(\d{1,2}\s*:\s*\d{2})',
               r'\1~\2', t)  # "12:00\n~\n13:30" â†’ "12:00~13:30"
    return t

def _collapse_korean_gaps(s: str) -> str:
    def fix_token(tok: str) -> str:
        core = tok.replace(" ", "")
        if re.fullmatch(r'[ê°€-í£]+', core) and 2 <= len(core) <= 5:
            return core
        return tok
    return " ".join(fix_token(t) for t in s.split())

def _extract_place_nodes(place_text: str) -> List[str]:
    clean = re.sub(r'<[^>]+>', ' ', place_text)  # ë³´ì¡°ì •ë³´ ì œê±°(ë¹„ê³ ë¡œ ì´ë™)
    clean = re.sub(r'\s+', ' ', clean).strip()
    parts = re.split(r'\s*(?:â†’|â†”|~)\s*', clean)  # ê²½ë¡œ êµ¬ë¶„ì
    nodes = [p.strip() for p in parts if p.strip()]
    return nodes

def _extract_headcount(block: str) -> Optional[Tuple[str, Tuple[int, int]]]:
    m = re.search(r'(\d{1,3}(?:,\d{3})*)\s*ëª…', block)
    if m:
        return m.group(1), m.span()
    for m2 in re.finditer(r'(\d{1,3}(?:,\\d{3})*|\\d{3,})', block):
        num = m2.group(1)
        tail = block[m2.end(): m2.end()+1]
        if tail == 'å‡º':  # ì¶œêµ¬ ë²ˆí˜¸ ì˜¤ê²€ì¶œ ë°©ì§€
            continue
        try:
            val = int(num.replace(',', ''))
            if val >= 100 or (',' in num):
                return num, m2.span()
        except ValueError:
            pass
    return None

def parse_pdf(pdf_path: str, ymd: Optional[Tuple[str, str, str]] = None) -> List[Dict[str, str]]:
    raw = extract_text(pdf_path) or ""
    text = _normalize_time_breaks(raw)

    rows: List[Dict[str, str]] = []
    matches = list(TIME_RE.finditer(text))
    for i, m in enumerate(matches):
        start_t = re.sub(r'\s+', '', m.group('start'))
        end_t   = re.sub(r'\s+', '', m.group('end'))

        start_idx = m.end()
        end_idx = matches[i+1].start() if i+1 < len(matches) else len(text)
        chunk = text[start_idx:end_idx].strip()

        # ì¸ì›
        head = _extract_headcount(chunk)
        if head:
            head_str, (h_s, h_e) = head
            head_clean = head_str.replace(',', '')
            before = chunk[:h_s]
            after  = chunk[h_e:]
        else:
            head_clean = ""
            before = chunk
            after  = ""

        # ì¥ì†Œ(ê²½ë¡œ) ë° ë³´ì¡°ì •ë³´
        place_block = before.strip()
        aux_in_place = " ".join(re.findall(r'<([^>]+)>', place_block))
        nodes = _extract_place_nodes(place_block)

        # ë¹„ê³  = ì¸ì› ì´í›„ ì”ì—¬ + ì¥ì†Œ ë³´ì¡°ì •ë³´
        remark_raw = " ".join(x for x in [after.strip(), aux_in_place.strip()] if x)
        remark = _collapse_korean_gaps(re.sub(r'\s+', ' ', remark_raw)).strip()

        # ì¥ì†Œ ì»¬ëŸ¼: 1ê°œë©´ ë¬¸ìì—´, 2ê°œ ì´ìƒì´ë©´ JSON ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´
        if len(nodes) == 0:
            place_col = ""
        elif len(nodes) == 1:
            place_col = nodes[0]
        else:
            place_col = json.dumps(nodes, ensure_ascii=False)

        row = {
            "ë…„": ymd[0] if ymd else "",
            "ì›”": ymd[1] if ymd else "",
            "ì¼": ymd[2] if ymd else "",
            "start_time": start_t,
            "end_time": end_t,
            "ì¥ì†Œ": place_col,
            "ì¸ì›": head_clean,   # ìˆ«ìë§Œ
            "ìœ„ë„": "[]",         # ì§€ì˜¤ì½”ë”©ì—ì„œ ì„¤ì •ë¨
            "ê²½ë„": "[]",         # ì§€ì˜¤ì½”ë”©ì—ì„œ ì„¤ì •ë¨
            "ë¹„ê³ ": remark,
        }
        rows.append(row)

    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VWorld ì§€ì˜¤ì½”ë”©
DEFAULT_VWORLD_KEY = os.environ.get("VWORLD_KEY", "46AEEE06-EE1D-3C1F-A4A4-E38D578695E8")
VWORLD_SEARCH_URL = "https://api.vworld.kr/req/search"
VWORLD_ADDR_URL   = "https://api.vworld.kr/req/address"

# ì„œìš¸ ê²½ê³„ ë°•ìŠ¤(ëŒ€ëµ)
SEOUL_BBOX = (37.413, 37.715, 126.734, 127.269)  # (lat_min, lat_max, lon_min, lon_max)

def in_seoul_bbox(lat: float, lon: float, bbox=SEOUL_BBOX) -> bool:
    if lat is None or lon is None:
        return False
    lat_min, lat_max, lon_min, lon_max = bbox
    return (lat_min <= lat <= lat_max) and (lon_min <= lon <= lon_max)

GU_PATTERN = re.compile(r"(ì¢…ë¡œêµ¬|ì¤‘êµ¬|ìš©ì‚°êµ¬|ì„±ë™êµ¬|ê´‘ì§„êµ¬|ë™ëŒ€ë¬¸êµ¬|ì¤‘ë‘êµ¬|ì„±ë¶êµ¬|ê°•ë¶êµ¬|ë„ë´‰êµ¬|ë…¸ì›êµ¬|ì€í‰êµ¬|ì„œëŒ€ë¬¸êµ¬|ë§ˆí¬êµ¬|ì–‘ì²œêµ¬|ê°•ì„œêµ¬|êµ¬ë¡œêµ¬|ê¸ˆì²œêµ¬|ì˜ë“±í¬êµ¬|ë™ì‘êµ¬|ê´€ì•…êµ¬|ì„œì´ˆêµ¬|ê°•ë‚¨êµ¬|ì†¡íŒŒêµ¬|ê°•ë™êµ¬)")
POLICE_TO_GU = {
    "ì¢…ë¡œì„œ": "ì¢…ë¡œêµ¬", "ë‚¨ëŒ€ë¬¸ì„œ": "ì¤‘êµ¬", "ì¤‘ë¶€ì„œ": "ì¤‘êµ¬", "ìš©ì‚°ì„œ": "ìš©ì‚°êµ¬", "ì„œëŒ€ë¬¸ì„œ": "ì„œëŒ€ë¬¸êµ¬",
    "ë§ˆí¬ì„œ": "ë§ˆí¬êµ¬", "ì˜ë“±í¬ì„œ": "ì˜ë“±í¬êµ¬", "ë™ì‘ì„œ": "ë™ì‘êµ¬", "ê´€ì•…ì„œ": "ê´€ì•…êµ¬", "ê¸ˆì²œì„œ": "ê¸ˆì²œêµ¬",
    "êµ¬ë¡œì„œ": "êµ¬ë¡œêµ¬", "ê°•ì„œì„œ": "ê°•ì„œêµ¬", "ì–‘ì²œì„œ": "ì–‘ì²œêµ¬", "ê°•ë‚¨ì„œ": "ê°•ë‚¨êµ¬", "ì„œì´ˆì„œ": "ì„œì´ˆêµ¬",
    "ì†¡íŒŒì„œ": "ì†¡íŒŒêµ¬", "ê°•ë™ì„œ": "ê°•ë™êµ¬", "ë™ëŒ€ë¬¸ì„œ": "ë™ëŒ€ë¬¸êµ¬", "ì„±ë¶ì„œ": "ì„±ë¶êµ¬", "ë…¸ì›ì„œ": "ë…¸ì›êµ¬",
    "ë„ë´‰ì„œ": "ë„ë´‰êµ¬", "ê°•ë¶ì„œ": "ê°•ë¶êµ¬", "ì„±ë™ì„œ": "ì„±ë™êµ¬", "ê´‘ì§„ì„œ": "ê´‘ì§„êµ¬", "ì€í‰ì„œ": "ì€í‰êµ¬",
}
def extract_gu_from_remark(remark: str) -> Optional[str]:
    if not remark:
        return None
    m = GU_PATTERN.search(remark)
    if m:
        return m.group(1)
    for k, gu in POLICE_TO_GU.items():
        if k in remark:
            return gu
    return None

# 'ì¢…ë¡œ' í‚¤ì›Œë“œ(ì§€ëª…/ê±´ë¬¼/ì—­ ë“±)
JONGNO_KEYWORDS = [
    "ì¢…ë¡œ", "ì¢…ë¡œêµ¬", "ì¢…ë¡œêµ¬ì²­",
    "ê´‘í™”ë¬¸", "ê´‘í™”ë¬¸ê´‘ì¥", "ì„¸ì¢…ë¬¸í™”íšŒê´€", "ì •ë¶€ì„œìš¸ì²­ì‚¬", "ê²½ë³µê¶",
    "ì‚¼ì²­ë™", "ì²­ìš´ë™", "ë¶€ì•”ë™", "ì¸ì‚¬ë™", "ìµì„ ë™", "ê³„ë™", "ì™€ë£¡ë™", "ì‚¬ì§ë¡œ", "ìœ¨ê³¡ë¡œ", "ìí•˜ë¬¸ë¡œ",
    "ê²½ë³µê¶ì—­", "ê´‘í™”ë¬¸ì—­", "ì•ˆêµ­ì—­", "ì¢…ê°ì—­", "ì¢…ë¡œ3ê°€ì—­", "ì¢…ë¡œ5ê°€ì—­",
    "í¥ì¸ì§€ë¬¸",  # ë™ëŒ€ë¬¸(í¥ì¸ì§€ë¬¸)
]

def normalize_no_space(s: str) -> str:
    return re.sub(r"\s+", "", s or "")

def text_has_any(text: str, keywords: List[str]) -> bool:
    t = normalize_no_space(text)
    return any(k in t for k in keywords)

def row_matches_jongno(r: Dict[str, str]) -> bool:
    # 1) ë¹„ê³ ì—ì„œ êµ¬ ì¶”ì •
    remark = r.get("ë¹„ê³ ", "") or ""
    if extract_gu_from_remark(remark) == "ì¢…ë¡œêµ¬":
        return True
    # 2) ë¹„ê³  í‚¤ì›Œë“œ
    if text_has_any(remark, JONGNO_KEYWORDS):
        return True
    # 3) ì¥ì†Œ(ë¬¸ìì—´/JSON) í‚¤ì›Œë“œ
    place_col = r.get("ì¥ì†Œ", "") or ""
    place_text = ""
    if place_col.strip().startswith("["):
        try:
            nodes: List[str] = json.loads(place_col)
        except json.JSONDecodeError:
            nodes = []
        place_text = " ".join(nodes)
    else:
        place_text = place_col
    if text_has_any(place_text, JONGNO_KEYWORDS):
        return True
    return False

def filter_rows_jongno(rows: List[Dict[str, str]]) -> List[Dict[str, str]]:
    return [r for r in rows if row_matches_jongno(r)]

# ë¹„ê³ ì—ì„œ ì¥ì†Œ í† í° í‚¤ì›Œë“œ ì¶”ì¶œ (ì§€ì˜¤ì½”ë”© í›„ë³´ ìƒì„±ìš©)
CONTEXT_TOKEN_PAT = re.compile(r"([ê°€-í£A-Za-z0-9]{2,}(?:ëŒ€ë¡œ|ë¡œ|ê¸¸|ê°€|ê´‘ì¥|ì‚¬ê±°ë¦¬|êµì°¨ë¡œ|ì—­|ë™|ê³µì›|ì²­ì‚¬|ë¹Œë”©|ì„¼í„°|ì£¼ë¯¼ì„¼í„°|íšŒê´€|í•™êµ|ëŒ€í•™|ë³‘ì›))")
def extract_context_tokens(remark: str) -> List[str]:
    if not remark:
        return []
    toks = CONTEXT_TOKEN_PAT.findall(remark)
    toks = [re.sub(r"\s+", "", t) for t in toks if len(t) <= 12]
    seen, out = set(), []
    for t in toks:
        if t and t not in seen:
            seen.add(t); out.append(t)
    return out

def _to_ascii_digits(s: str) -> str:
    mapping = {ord('ï¼'):'0',ord('ï¼‘'):'1',ord('ï¼’'):'2',ord('ï¼“'):'3',ord('ï¼”'):'4',
               ord('ï¼•'):'5',ord('ï¼–'):'6',ord('ï¼—'):'7',ord('ï¼˜'):'8',ord('ï¼™'):'9',ord('ã€‡'):'0'}
    return (s or "").translate(mapping)

def _insert_space_between_kor_engnum(s: str) -> str:
    s = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', s or "")
    s = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', s)
    return s

def normalize_tokens_basic(place: str) -> str:
    t = _to_ascii_digits(place or "")
    t = t.replace("å‡ºå£", "ì¶œêµ¬").replace("å‡º", "ì¶œêµ¬").replace("å£", "ì¶œêµ¬")
    t = _insert_space_between_kor_engnum(t)
    t = re.sub(r'(\d+)\s*(?:ë²ˆ)?\s*(?:ì¶œ|ì¶œêµ¬)\b', r'\1ë²ˆ ì¶œêµ¬', t)
    t = re.sub(r'(\d+)\s*ë²ˆ\s*ì¶œêµ¬', r'\1ë²ˆ ì¶œêµ¬', t)
    t = re.sub(r'\s{2,}', ' ', t).strip()
    return t

def build_query_candidates(place: str, remark: str) -> List[str]:
    base = normalize_tokens_basic(place)
    cand: List[str] = []
    seen = set()

    def add(x: str):
        xx = (x or "").strip()
        if xx and xx not in seen:
            seen.add(xx); cand.append(xx)

    add(base)

    # ì—­ ì¶œêµ¬ íŒ¨í„´: "ì„œìš¸ì—­ 12ë²ˆ ì¶œêµ¬"
    m = re.search(r'(.*?ì—­)\s*(\d+)\s*ë²ˆ\s*ì¶œêµ¬', base)
    if m:
        st, num = m.group(1).strip(), m.group(2)
        add(f"{st} {num}ë²ˆ ì¶œêµ¬"); add(f"{st} {num}ë²ˆì¶œêµ¬"); add(f"{st} {num} ì¶œêµ¬"); add(st)

    # PB í™•ì¥
    if re.search(r'\bPB\b', base, re.IGNORECASE) or 'PB' in base:
        stub = re.sub(r'\bPB\b', '', base, flags=re.IGNORECASE).strip()
        add(f"{stub} íŒŒì¶œì†Œ"); add(f"{stub} ì§€êµ¬ëŒ€"); add(f"{stub} ê²½ì°°ë°•ìŠ¤")

    # 'ì‚¼ê°ì§€' ë³´ê°•
    if 'ì‚¼ê°ì§€' in base and 'ì—­' not in base:
        add("ì‚¼ê°ì§€ì—­"); add("ì‚¼ê°ì§€ ì‚¬ê±°ë¦¬"); add("ì‚¼ê°ì§€ êµì°¨ë¡œ")

    # í”„ë¦¬í”½ìŠ¤
    gu = extract_gu_from_remark(remark or "")
    prefixes = []
    if gu:
        prefixes.append(f"ì„œìš¸ {gu}")
    prefixes.append("ì„œìš¸")

    # ë¹„ê³  í† í° ê²°í•©
    ctx_toks = extract_context_tokens(remark or "")

    expanded = []
    for q in cand:
        expanded.append(q)
        for pfx in prefixes:
            expanded.append(f"{pfx} {q}")

    for tok in ctx_toks:
        expanded.append(tok)
        expanded.append(f"{tok} {base}")
        expanded.append(f"{base} {tok}")
        for pfx in prefixes:
            expanded.append(f"{pfx} {tok}")
            expanded.append(f"{pfx} {tok} {base}")
            expanded.append(f"{pfx} {base} {tok}")

    out, seen2 = [], set()
    for q in expanded:
        q2 = re.sub(r'\s{2,}', ' ', q).strip()
        if q2 and q2 not in seen2:
            seen2.add(q2); out.append(q2)
    return out

def _vworld_search_place(query: str, key: str, session: requests.Session,
                         context_gu: Optional[str] = None,
                         restrict_seoul: bool = True) -> Optional[Tuple[float, float]]:
    params = {
        "service": "search", "request": "search", "version": "2.0",
        "format": "json", "size": 7, "page": 1, "type": "place", "query": query, "key": key
    }
    try:
        r = session.get(VWORLD_SEARCH_URL, params=params, timeout=6)
        if r.status_code != 200:
            return None
        data = r.json()
        items = (((data or {}).get("response") or {}).get("result") or {}).get("items", [])
        best = None
        best_score = -1
        for it in items:
            pt = (it.get("point") or {})
            x = pt.get("x"); y = pt.get("y")
            if x is None or y is None:
                coords = ((it.get("geometry") or {}).get("coordinates"))
                if isinstance(coords, list) and len(coords) >= 2:
                    x, y = coords[0], coords[1]
            try:
                lon = float(x); lat = float(y)
            except Exception:
                continue

            addr_obj = it.get("address") or {}
            if isinstance(addr_obj, dict):
                addr = addr_obj.get("road") or addr_obj.get("parcel") or ""
            else:
                addr = str(addr_obj or "")
            title = (it.get("title") or "")

            score = 0
            if "ì„œìš¸" in addr or "Seoul" in addr:
                score += 10
            if context_gu and context_gu in addr:
                score += 4
            qkey = re.sub(r"\s+", "", query)
            if qkey and re.sub(r"\s+", "", title).find(qkey) >= 0:
                score += 2
            if in_seoul_bbox(lat, lon):
                score += 5

            if restrict_seoul:
                if ("ì„œìš¸" not in addr and "Seoul" not in addr) and (not in_seoul_bbox(lat, lon)):
                    continue

            if score > best_score:
                best_score = score
                best = (lat, lon)

        return best
    except requests.RequestException:
        return None

def _vworld_address_coord(addr: str, key: str, session: requests.Session, addr_type: str) -> Optional[Tuple[float, float]]:
    params = {
        "service": "address", "request": "getCoord", "version": "2.0",
        "format": "json", "crs": "EPSG:4326", "type": addr_type, "address": addr, "key": key
    }
    try:
        r = session.get(VWORLD_ADDR_URL, params=params, timeout=6)
        if r.status_code != 200:
            return None
        data = r.json()
        res = (data.get("response") or {}).get("result") or {}
        pt = (res.get("point") or {})
        x, y = pt.get("x"), pt.get("y")
        if x is None or y is None:
            return None
        lon = float(x); lat = float(y)
        if not in_seoul_bbox(lat, lon):
            return None
        return (lat, lon)
    except requests.RequestException:
        return None
    except Exception:
        return None

def geocode_vworld(query: str, key: str, session: requests.Session,
                   context_gu: Optional[str] = None,
                   restrict_seoul: bool = True) -> Optional[Tuple[float, float]]:
    q = (query or "").strip()
    if not q:
        return None
    hit = _vworld_search_place(q, key, session, context_gu=context_gu, restrict_seoul=restrict_seoul)
    if hit:
        return hit
    hit = _vworld_address_coord(q, key, session, "road")
    if hit:
        return hit
    hit = _vworld_address_coord(q, key, session, "parcel")
    return hit

def geocode_rows_inplace(rows: List[Dict[str, str]], vworld_key: str,
                         restrict_seoul: bool = True, sleep_sec: float = 0.15):
    session = requests.Session()
    cache: Dict[str, Optional[Tuple[float, float]]] = {}

    for r in rows:
        remark = r.get("ë¹„ê³ ", "") or ""
        gu = extract_gu_from_remark(remark) or None

        # ì¥ì†Œ ë¦¬ìŠ¤íŠ¸ í™•ë³´: ë¬¸ìì—´(1ê°œ) ë˜ëŠ” JSON ë°°ì—´ ë¬¸ìì—´
        place_col = r.get("ì¥ì†Œ", "") or ""
        if place_col.strip().startswith("["):
            try:
                nodes: List[str] = json.loads(place_col)
            except json.JSONDecodeError:
                nodes = []
        else:
            nodes = [place_col] if place_col.strip() else []

        lat_list: List[Optional[float]] = []
        lon_list: List[Optional[float]] = []

        for p in nodes:
            base = str(p or "")
            cleaned = re.sub(r"[ï¼ˆ(].*?[ï¼‰)]", "", base).strip()
            candidates = build_query_candidates(cleaned or base, remark)

            hit: Optional[Tuple[float, float]] = None
            for q in candidates:
                if q in cache:
                    hit = cache[q]
                else:
                    hit = geocode_vworld(q, vworld_key, session, context_gu=gu, restrict_seoul=restrict_seoul)
                    cache[q] = hit
                    time.sleep(sleep_sec)
                if hit:
                    lat, lon = hit
                    if (not restrict_seoul) or in_seoul_bbox(lat, lon):
                        break
                    else:
                        hit = None
                        continue

            if hit:
                lat, lon = hit
                lat_list.append(lat); lon_list.append(lon)
            else:
                lat_list.append(None); lon_list.append(None)

        r["ìœ„ë„"] = json.dumps(lat_list, ensure_ascii=False)
        r["ê²½ë„"] = json.dumps(lon_list, ensure_ascii=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV ì¶œë ¥
def write_csv(rows: List[Dict[str, str]], out_path: str) -> None:
    fields = ["ë…„","ì›”","ì¼","start_time","end_time","ì¥ì†Œ","ì¸ì›","ìœ„ë„","ê²½ë„","ë¹„ê³ "]
    ensure_dir(os.path.dirname(out_path) or ".")
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        for r in rows:
            w.writerow(r)

def merge_and_save_csv(rows: List[Dict[str, str]], out_path: str) -> None:
    """ê¸°ì¡´ CSVê°€ ìˆìœ¼ë©´ ë³‘í•©/ë³´ê°•, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    fields = ["ë…„","ì›”","ì¼","start_time","end_time","ì¥ì†Œ","ì¸ì›","ìœ„ë„","ê²½ë„","ë¹„ê³ "]
    ensure_dir(os.path.dirname(out_path) or ".")

    new_df = pd.DataFrame(rows, columns=fields).fillna("")

    if os.path.exists(out_path):
        old_df = pd.read_csv(out_path, dtype=str).fillna("")

        for _, new_row in new_df.iterrows():
            matched = False
            for idx, old_row in old_df.iterrows():
                if (old_row["start_time"] == new_row["start_time"] and
                    old_row["end_time"] == new_row["end_time"] and
                    old_row["ì¥ì†Œ"] == new_row["ì¥ì†Œ"]):
                    matched = True
                    # ë³´ê°•: ê¸°ì¡´ì— ë¹„ì–´ ìˆìœ¼ë©´ ìƒˆ ë°ì´í„°ë¡œ ì±„ì›€
                    for col in fields:
                        if old_row[col] == "" and new_row[col] != "":
                            old_df.at[idx, col] = new_row[col]
                    break
            if not matched:
                old_df = pd.concat([old_df, pd.DataFrame([new_row])], ignore_index=True)

        final_df = old_df
    else:
        final_df = new_df

    final_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {out_path} (ì´ {len(final_df)}í–‰)")
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ë¶€
def main():
    ap = argparse.ArgumentParser(description="SMPA 'ì˜¤ëŠ˜ì˜ ì§‘íšŒ' PDF â†’ CSV (íŒŒì‹±+VWorld ì§€ì˜¤ì½”ë”©+ì¢…ë¡œ í•„í„°)")
    ap.add_argument("--pdf", default=None, help="ì…ë ¥ PDF ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ì˜¤ëŠ˜ì ê²Œì‹œê¸€ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)")
    ap.add_argument("--out", default=None, help="ì „ì²´ CSV ì €ì¥ ê²½ë¡œ(ê¸°ë³¸: ./ì§‘íšŒì •ë³´.csv)")
    ap.add_argument("--attachments-dir", default="attachments", help="ìë™ ë‹¤ìš´ë¡œë“œ ì‹œ PDF ì €ì¥ í´ë”")
    ap.add_argument("--vworld-key", default=DEFAULT_VWORLD_KEY, help="VWorld API Key (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜ VWORLD_KEY ë˜ëŠ” ë‚´ì¥ ê¸°ë³¸ê°’)")
    ap.add_argument("--no-seoul-filter", action="store_true", help="ì§€ì˜¤ì½”ë”© ì‹œ ì„œìš¸ ê²½ê³„ ë°•ìŠ¤ í•„í„° ë„ê¸°")
    ap.add_argument("--geocode-sleep", type=float, default=0.15, help="ì§€ì˜¤ì½”ë”© ìš”ì²­ ê°„ ëŒ€ê¸°(ì´ˆ)")
    args = ap.parse_args()

    ymd: Optional[Tuple[str, str, str]] = None

    # PDF ê²½ë¡œ ê²°ì • + ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
    if args.pdf:
        pdf_path = args.pdf
        if not os.path.isfile(pdf_path):
            raise SystemExit(f"ì…ë ¥ PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    else:
        pdf_path, title_text = download_today_pdf_with_title(out_dir=args.attachments_dir)
        print(f"[ì •ë³´] ì˜¤ëŠ˜ì PDF ë‹¤ìš´ë¡œë“œ: {pdf_path}")
        ymd = extract_ymd_from_title(title_text)
        if ymd:
            print(f"[ì •ë³´] ê²Œì‹œê¸€ ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ: {ymd[0]}-{ymd[1]}-{ymd[2]}")
        else:
            print("[ê²½ê³ ] ì œëª©ì—ì„œ ë‚ ì§œ(YYMMDD)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë…„/ì›”/ì¼ì€ ê³µë€ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.")

    # íŒŒì‹±
    rows = parse_pdf(pdf_path, ymd=ymd)

    # ì§€ì˜¤ì½”ë”©
    restrict = not args.no_seoul_filter
    if args.vworld_key:
        try:
            geocode_rows_inplace(rows, vworld_key=args.vworld_key,
                                 restrict_seoul=restrict, sleep_sec=args.geocode_sleep)
        except Exception as e:
            print(f"âš ï¸ ì§€ì˜¤ì½”ë”© ì‹¤íŒ¨(ê±´ë„ˆëœ€): {e}")
    else:
        print("â„¹ï¸ VWorld í‚¤ê°€ ì—†ì–´ ì§€ì˜¤ì½”ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤. --vworld-key ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ VWORLD_KEYë¥¼ ì§€ì •í•˜ì„¸ìš”.")

    # ì§‘íšŒ ë‚ ì§œ ê¸°ë°˜ íŒŒì¼ëª…
    if ymd:
        date_str = f"{ymd[0]}-{ymd[1]}-{ymd[2]}"
    else:
        date_str = datetime.now().strftime("%Y-%m-%d")

    ensure_dir("data")
    out_all = os.path.join("data", f"ì§‘íšŒ_ì •ë³´_{date_str}.csv")
    out_jongno = os.path.join("data", f"ì§‘íšŒ_ì •ë³´_{date_str}_ì¢…ë¡œ.csv")

    # ì €ì¥
    merge_and_save_csv(rows, out_all)
    rows_jongno = filter_rows_jongno(rows)
    merge_and_save_csv(rows_jongno, out_jongno)

    print(f"[ì™„ë£Œ] ì „ì²´ CSV ì €ì¥: {out_all} (ì´ {len(rows)}í–‰)")
    print(f"[ì™„ë£Œ] ì¢…ë¡œ í•„í„° CSV ì €ì¥: {out_jongno} (ì´ {len(rows_jongno)}í–‰)")


if __name__ == "__main__":
    main()
